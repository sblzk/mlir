

\documentclass{acm_proc_article-sp}
\usepackage{algorithm}
\usepackage{setspace}
\usepackage{algorithmic}
\begin{document}

\title{Investigating the effects of bootstrapping and active learning in an online reinforcement learning setting to rank for information retrieval}


\numberofauthors{2} 
\author{
\alignauthor
Mayank Kejriwal\\
       \affaddr{University of Texas at Austin}\\
       \email{kejriwal@cs.utexas.edu}
% 2nd. author
\alignauthor
Sam Blazek\\
       \affaddr{University of Texas at Austin}\\
       \email{sam.blazek@gmail.com}
}

\date{23 October 2013}


\maketitle
\begin{abstract}
As the development of effective and efficient information retrieval systems relies increasingly on automated techniques, ranking procedures based on machine learning methods have drawn much attention. \emph{Online learning to rank} algorithms permit retrieval systems to learn directly from live user activity, but offer several challenges. Most importantly, the system must learn from the user without interfering with his or her search activities and decreasing user satisfaction. This requires a balanced approach to both experimentation and tuning (\emph{exploration}) and service provision (\emph{exploitation}). We examine recent applications of \emph{reinforcement learning} in \emph{listwise} and \emph{pairwise} approaches to learning to rank, and prototype and evaluate new \emph{active learning} and \emph{bootstrapping} strategies in which ranked results presented to the user are tailored to gather more valuable information with less interference with users' retrieval activities. Our hypothesis is that this reduces the amount of exploration required to produce results of equivalent quality to previous reinforcement learning implementations. We empirically investigate this hypothesis using a recently developed online simulation framework.
\end{abstract}


\keywords{Information Retrieval, Reinforcement learning, active learning} 

\section{Introduction}
As large quantities of information continue to be exposed on the web, and retrieval systems get more complicated to serve the demands of a global base of users, the need for effective automation of key parts of the retrieval process is pressing. In the information retrieval community, the cost and difficulty of obtaining relevance judgments is well documented. Relevance judgment sets for large collections of documents are often incomplete, in the sense that not every document in the collection has been judged as relevant or non-relevant. The dimensions of the problem only conflate as issues of what constitutes relevance, or even the granularity of relevance, come into play. \\ \\
On the other hand, implicit feedback and preferences of users can often be ascertained using click log data. This data is usually noisy, that is, there is always a possibility that the click model assumed by the system is not the one that fits the user click model. Because of accidental clicks or variance of preference among users, deducing relevance judgments of documents based on a preference function is susceptible to flawed conclusions. Despite this observation, implicit feedback still provides useful data about the preferences of the user much of the time. The need of the day, then, is to develop ranking algorithms that can satisfy the needs of such users without requiring much manual fine-tuning or supervision, both of which mandate a high-confidence relevance judgment set. \\ \\
In the framework of reinforcement learning \cite{rl}, such systems that are constantly getting reinforced by feedback from the user face a natural dilemma. On the one hand, the system can choose to be \emph{exploitative} i.e. it returns results or performs actions that have been found to be the best thus far. By being exploitative, however, the system is taking the risk of not discovering actions, or navigating a space of solutions, that \emph{might} yield higher rewards had the system known about them. In other words, by being exploitative, the system chooses to be conservative and not \emph{explore} unknown regions of the solution space that could potentially be gold mines. \emph{Exploration} in reinforcement learning is, thus, the antithesis of \emph{exploitation}. A system explores, not with the goal of maximizing present reward, but with an intent to seek out solutions that will pay off in the long run with higher rewards. On the other hand, exploitation presents the solution that has the highest guaranteed payoff in the present. Finding a balance between exploration and exploitation is an important problem in reinforcement learning. In other words, the present should not always be sacrificed for a future that may never arrive, but to cling to the present set of solutions may be to risk significant sub-optimality.\\ \\
Reinforcement learning is a very general formulation that is well suited to many different tasks and domains. In information retrieval, exploring the solution space amounts to presenting documents to users for which the system has no score or judgement. Exploitation in information retrieval amounts to always retrieving and displaying those documents for which the system has a high score. If the system is perfect or an oracle, then pure exploitation is obviously the strategy of choice. In practice, this is not the case. The parameters of the system overfit on the limited data available to the system. Because of this, they are not useful for judging parts of the solution space about which they have no feedback. In a recent work, Hofmann et al. showed that striking a correct balance between exploration and exploitation is the strategy of choice for maximizing long term returns \cite{hofmann}. For the exploration part of the system, their model was \emph{random}. That is, the system explored by sampling documents from the entire set randomly and presenting to the user for feedback. \\ \\
In a separate, earlier work, Tian and Lease showed that active learning approaches can be useful for efficiently reducing uncertainty about the solution space \cite{aibo}. The benefits of active learning have also been demonstrated in other domains, and corroborated by other researchers in the information retrieval community as well \cite{active2}. The idea is to explore the unknown solution space \emph{intelligently} by presenting only those examples to users that are expected to maximize information gain. \\ \\
In this work, we propose to combine the benefits of both reinforcement learning and of active learning \emph{within} the exploratory aspect of reinforcement learning. In other words, rather than doing random exploration, we will be using an active learning approach. \\ \\
We will also demonstrate the effects of bootstrapping an online system, and the susceptibility of the system to bootstrapping noise. By bootstrapping, we mean that we will train the initial parameters of the online system by exposing a small set of documents and relevance labels rather than starting with randomly generated initial parameters. In this paradigm, we are investigating the benefits of offline learning that precedes the online phase. We test the hypothesis that even very limited bootstrapping can drastically impact the convergence properties of online algorithms in the reinforcement learning framework. 
\section{Related Work}
Although our primary contributions (subsequently detailed) are original, there is some work that forms a precedent for ours. We list the main components of our research, and describe the focal work that biased us towards using or investigating those components:
\begin{itemize}
\item \emph{Learning from implicit feedback}: The work by Joachims \cite{joachims} addressed the problem of using clickthrough data to train a system. The system used in that work was a Support Vector Machine (SVM). Building on this work, several others have showed that it is possible to learn reliably in an online framework i. e. from implicit feedback. Just like in the work by Hofmann et al. \cite{hofmann}, we adopt the basic pairwise approach in this work. Other works have addressed learning from implicit feedback as well, but the paper by Joachims \cite{joachims} seems to have been heavily cited, and highly influential in the area. We chose this work as central for this aspect of the research, therefore. In a similar vein as Hofmann et al. \cite{hofmann}, we also use a listwise algorithm. Our focal work for a listwise algorithmic approach is the SoftRank system by Taylor et al. \cite{taylor}. 
\item \emph{Reinforcement learning}: Since the work by Sutton and Barto \cite{rl}, reinforcement learning has seen tremendous research in the machine learning community. The basic goal of reinforcement learning is to allow the \emph{agent} to be \emph{reinforced} by the \emph{environment}. In other words, the goal of the agent is to maximize the rewards received from the environment. The naive way to do this is to always \emph{exploit} what it knows already and receive a guaranteed reward at each time step. However, if large swathes of the solution space are unexplored, the performance of an exploitative agent would be highly sub-optimal. Hence, the agent needs to balance \emph{exploration} with \emph{exploitation} to maximize reward in the long run. In their work, Hofmann et al. modeled the 'long run' as a discounted sequence of rewards over an infinite time horizon (in practice, the horizon ended after 1000 iterations in their experiments). One key reinforcement learning algorithm is $\epsilon$-greedy RL. Intuitively, the parameter $0.0 \leq \epsilon \leq 1.0$ controls the exploration-exploitation tradeoff. We are not aware of any works before Hofmann et al. \cite{hofmann} that use rigorous reinforcement learning algorithms in information retrieval. Hence, their work is our focal point for running experiments, formulating baseline and methodology, as well as guidance for future work and improvement. Hence, their work is arguably the focal reference for the overall research goals of this paper.
\item \emph{Active learning}: Active learning is also a popular approach in the machine learning community; however, it is different from reinforcement learning in several ways. Unlike reinforcement learning, active learning has no formal provision for exploitation although practical systems use active learning in conjunction with some exploitation scheme. Instead, active learning dictates the rules of exploration. Its goal is to ensure that the \emph{cost} of exploration is small. In other words, by intelligently exploring the unknown solution space, it is able to map it out with minimal user effort. One influential work in active learning in information retrieval was by Xu et al. \cite{active2}. They used an active learning method that used criteria of relevance, document diversity and document density to make decisions on what documents to present to the user. However, the focal reference for us in this area is the more recent work by Tian and Lease \cite{aibo}. The reason why we find their work attractive is because they are able to abstract, successfully, the choice of features from the active learning itself and demonstrate the benefits of active learning as a standalone framework. They use a generic SVM to find vectors in the feature space that have maximum information gain by using a number of criteria, including the distance of the vector from the hyperplane surface of the SVM. Adapting their approach to enable active learning in the exploratory phase of reinforcement learning will be a key contribution of our work. 
\end{itemize}
\section{Contributions}
In this paper, we propose the following contributions. Note that specific milestones and additional details are provided in a subsequent section. In this section, we merely aim to enumerate our high level goals for the final deliverables.
\begin{itemize}
\item An implementation of an \emph{online} pairwise ranking algorithm, with user activity simulated using click models. The pairwise algorithm will rely on implicit feedback, similar to the approach by Joachims \cite{joachims}. However, the pairwise ranking algorithm will be set in a reinforcement learning framework \cite{rl}, shown to be an effective way of balancing exploration and exploitation in a recent work by Hofmann et al. \cite{hofmann}. Our original contribution will be to place the \emph{exploration} part of the setting in an active learning framework, instead of choosing documents randomly (as in \cite{hofmann}). In a series of experiments, Tian and Lease showed that active learning can be highly efficient with respect to minimizing user labeling effort in information retrieval \cite{aibo}. We are currently not aware of any work (in information retrieval) that attempts to combine the benefits of both reinforcement learning and active learning. 
\item We will also repeat the above experiments for a \emph{listwise} ranking algorithm (also online). An example of a listwise ranking algorithm, called SoftRank, is presented by Taylor et al. \cite{taylor}. Again, rather than using random measures for the exploration part of reinforcement learning, we will intelligently choose the space that we want to explore.
\item We will study the effects of limited supervised bootstrapping for the above approaches. A key observation upon surveying the current literature was that many authors tended to either adopt a supervised approach (i.e. an offline training and validation phase) or an approach that started with parameters that were random but that converged to acceptable values as more feedback was elicited from the user. For example, Hofmann et al. started with random, normalized weight vectors for their reinforcement learning algorithms and updated these vectors as implicit feedback was obtained \cite{hofmann}. Tian and Lease effectively do something similar, but in an active learning setting \cite{aibo}. However, we would like to consider a compromise, based on practical needs. Specifically, we propose a \emph{bootstrapping} phase before the online learning. Our hypothesis is that even if we generate our parameters (like the weight vector) based initially on a small set of training samples, and then use those parameters to start off the online session, we will witness considerable gains. If this effect is experimentally observed, a related question is how much bootstrapping is required and whether the system is robust to noise in that phase. One key contribution of this paper will be to answer some of these questions for the algorithms and settings above.  
\end{itemize}

\section{Risks}
For pairwise learning, what if the $\epsilon$-greedy random selection strategy of Hofmann et al. \cite{hofmann} actually works better than, or at least as well as, the \emph{active learning} exploratory document selection criterion? Similarly, for listwise learning, what if the original dueling bandit gradient descent algorithm outperforms our modified approach? Either result would ostensibly argue against the assumed advantages of active learning and non-random exploratory document or list selection, and if we encounter either, we will run additional experiments with controlled modifications to our original active learning algorithms in order to identify and report on the factors that counter our hypothesis.\\ \\
Another issue could be that results are inconsistent or inconclusive across different experiments, which could prevent us from making firm conclusions regarding our findings. We use the cumulative NDCG metric to score our runs and perform basic descriptive statistical techniques such as student's \emph{t} test to compare against baseline scores. If significance is not established in all experiments in a manner consistent with our hypothesis, then we must investigate the factors that may have made our hypothesis incorrect or difficult to test. In particular, finding unexpected differences between our listwise and pairwise algorithms' performance would require further investigation and,  potentially, modification of one or both algorithms. It is a common adage that more data can always improve a learning model. However, several datasets are poorly formatted or otherwise unsuitable for our task. In addition, due to time constraints we limit our datasets to the TREC 2003 and 2004 tracks of LETOR 2.0 \footnote{http://research.microsoft.com/en-us/um/beijing/projects/letor/Letor2.0/dataset.aspx}. These datasets from Microsoft are designed to facilitate research in learning to rank. Therefore, our datasets limit the generalizability of our findings as they did in the work by Hofmann et al. \cite{hofmann}.

\section{Current Progress}
In this section, we list our current progress per the milestones above. 

\subsection{Click Model}
\begin{table}
\caption{Overview of click models}
    \begin{tabular}[t]{ |p{2.0cm} | p{1.3cm} | p{1.3cm} | p{1.3cm} | p{1.3cm} |}
    \hline
    Model & $p(c|R)$ & $p(c|NR)$ & $p(s|R)$ & $p(s|NR)$ \\ \hline
    Perfect & $1.0$ & $0.0$ & $0.0$ & $0.0$  \\ \hline
Navigational & $0.95$ & $0.05$ & $0.9$ & $0.2$   \\ \hline
Informational & $0.9$ & $0.4$ & $0.5$ & $0.1$    \\ \hline
    \end{tabular}
\label{click}
\end{table}
As in the work by Hofmann et al. \cite{hofmann} we have experimented with three different click models: \emph{perfect}, \emph{navigational} and \emph{informational}. The click and stop probabilities, conditioned on whether the document was relevant or non-relevant, are outlined in Table \ref{click}. The perfect click model essentially assumes the user to be an oracle. Specifically, the user always clicks on a relevant document, and never clicks on a non-relevant one. The user never stops till the end of the list is encountered. Hence stop probabilities are zero. The informational and navigational models are based on empirically validated studies emulating typical user behavior in web search \cite{broder, guo1, guo2}. The navigational model is a slightly noisier version of the perfect model as far as clicking probabilities are concerned. The difference between probabilities is still large. However, the stopping probability, given a document is relevant is close to $1.0$. This is because in a navigational task, users are looking for a very specific result and they stop once they find that result. In an informational task, the importance of recall is more emphasized and users are less certain about when to stop and when to click. All four probabilities show less deviation from each other than in the other two cases.\\ \\
\begin{algorithm}[t]
\caption{SimulateProbability($p$)}
\begin{spacing}{0.9}
\begin{algorithmic}

\STATE \textbf{Input :} \begin{itemize}
\item Probability $p$ (limited to two significant digits)
\item Integer Random number generator $g$  
 \end{itemize}
\STATE \textbf{Output :} \begin{itemize}
\item Boolean value \emph{True} with probability $p$
 \end{itemize}
\STATE \textbf{Method :}
\begin{enumerate}
\STATE{Initialize Set $S$ to be empty}
\STATE{$range:=[0,100)$}
\STATE{Randomly generate $100-(p*100)$ integers in $range$ using $g$ and add to $S$}
\STATE{Randomly generate an integer $q$ in $range$ using $g$}
\IF{$q \in S$}
\STATE{Terminate program and output \emph{False}}
\ENDIF
\STATE{Terminate program and output \emph{True}}
\end{enumerate}
\end{algorithmic}
\end{spacing}
\label{clicksimulate}
\end{algorithm}
One of the first issues we encountered was how to simulate probability distributions. Specifically, given a probability we wanted to return a boolean value (for example, if we were simulating a user in a informational setting, and we knew a priori the document was relevant, we would return \emph{True} with probability $0.9$ per Table \ref{click}). We settled on a scheme that was simple and approximate but is shown to yield good results in practice. The pseudocode for our algorithm is shown in Algorithm \ref{clicksimulate}. Essentially, the algorithm works by using a random number generator (ideally, this would be perfectly random, but is usually only pseudo-random in actual implementations) to independently generate $100-(p*100)$ integers in the range of $[0,100)$ where $p$ is the probability with which we want to return \emph{True} and only has two significant digits (the data in Table \ref{click} also assumes this). Next, we sample a single integer, again from the range $[0,100)$. If this integer is equal to any of the integers generated in the previous step, we return \emph{False} else we return \emph{True}. To test the algorithm, we called it 1000, 10000 and 100000 times with $p=0.9$. For the first two cases, the algorithm returned true around $90.2$ percent of the time ($90.1$ percent for the last case). Hence, there is less than $0.2$ percent error which further decreases as we sample more often. We ran a similar experiment with other probability values and the results were validated.  

\subsection{Metrics}

We implemented more than we intended on this front. Originally, we were intending to only implement NDCG@10 by now and implement the other metrics later; currently, however, we have NDCG@10, MAP and Precision@10 implemented. We have conducted experiments on them and have determined them to be functioning as expected. 

\subsection{Data}

We have succeeded in locating and parsing the datasets we intended to work on (Letor 2.0 TREC 2003 and 2004) into appropriate data structures. This was one of the first things we got set up in the code infrastructure. Note that we are implementing everything in Java. 

\subsection{Pairwise Algorithms}

There are three algorithms that were supposed to be implemented for the pairwise case of implicit feedback. These, respectively, were the baseline case where the weight vector is learned offline using stochastic gradient descent, a click model and feedback \cite{joachims}, the baseline case supplemented to be online in a reinforcement learning framework as proposed originally in \cite{hofmann} and our proposed approach where we supplement the exploration phase of the reinforcement learning in the previous case with active learning. \\ \\
We have succeeded in setting up the code infrastructure for all three algorithms and initial tests show promising evidence that the baseline case is working, at least on the perfect click model. Our numbers look reasonable but more testing will be required to see if subtle bugs are present in the code. We are currently in the process of doing so.

\section{Experiments}
The experimental results reveal many notable trends.  We discuss findings along different metrics and click models separately.

\subsection{Pairwise Algorithms}
\subsubsection{Perfect Click Model}
For the perfect click model, the baseline model is outperformed by both the reinforcement learning model and the active learning model for NDCG@10, Precision@10, and Mean Average Precision (MAP) for both the TD2003 and the TD2004 datasets.  In terms of MAP measurements, the baseline was outperformed not only by both of the other models, but also at every level of r for the TD2004 dataset, and by most values of r for the TD2003 dataset.  \\ \\
In the TD2004 dataset, the highest overall values for every metric came from the reinforcement learning model with r=1 (purely exploratory).  This is consistent with the NDCG@10 and Precision@10 measurements for TD2003 as well, but not with the MAP values, where the active learning model outperformed the reinforcement model for values of r=0.6 and higher.  The single highest MAP value for the TD2003 experiments comes from the active learning model at r=1.   \\ \\
Looking at both learning models, 9 of the 12 perfect click model metrics have maxima at r=1, where exploration is maximized.  This is inconsistent with the findings of Hofmann et al [cite], whose best performance on TD2003 and TD2004 is achieved when minimizing exploration.  \\ \\

\subsubsection{Navigational Click Model}
The navigational click model provided slightly worse results than those of the perfect click model.  This is consistent with the findings of previous Learning to Rank literature [cite Hofmann et al].\\
Interestingly, when using the navigational click model, the reinforcement learning model performed best at a balanced value of exploration (r=0.4), rather than at an extreme, for all metrics of both TD2003 and TD2004 except for Precision@10 in TD2003, where the best performance was found at r=1.  The active learning model, by comparison, had more scattered maxima:  for TD2004, performance in terms of NDCG@10 and Precision@10 was best at r=1, MAP was best at r=0.2; for TD2003, all models performed best at r=0.6.   The active learning outperformed the reinforcement learning model in terms of both NDCG@10 and MAP when tested using TD2003, and also bested the MAP and Precision@10 scores of the reinforcement learning model when tested using TD2004.  \\ \\
Overall, the active learning model appeared to show greater improvement over the reinforcement learning model when applying the navigational click model than when applying the perfect click model. 

\subsubsection{Informational Click Model}
The informational click model’s experimental results are roughly comparable to those of the navigational click model, and slightly worse than the perfect click model.  However, the informational click model’s best results have the greatest variance in values of r between the three metrics and two datasets.   \\ \\
For the TD2003 dataset, the reinforcement learning model maximizes NDCG@10 and Precision@10 values at r=0.4 and maximized MAP at r=0.2.  The active learning model maximizes performance for NDCG@10 and MAP at r=0.4 and Precision@10 for r=0.2.  These findings suggest a balanced exploration/exploitation approach is best for TD2003, which is generally consistent with Hofmann et al [cite].  The active learning model outperforms the reinforcement learning model in terms of both NDCG@10 and Precision@10, but not MAP. \\ \\
For the TD2004 dataset, the reinforcement learning model outperforms the active learning model for all metrics.  The reinforcement learning model performs best when totally exploratory for NDCG@10 and Precision@10, and totally exploitative for MAP.  The active learning model performs best when totally exploitative for NDCG@10 and Precision@10, and highly exploratory (r=0.8) for MAP.   

\subsection{Listwise Algorithm}
The Listwise baseline experiment provided measurements slightly higher than those of the Pairwise experiments.  This is consistent with Hofmann et al [cite].  However, the moving average experiment was not able to improve on this baseline for any test, metric, dataset, or fold.  \\ \\
There are several possible explanations for this.  One is that the datasets are not large enough to evaluate the differences between the listwise algorithms, which do not gather information directly from particular documents and, as a result, may struggle to effectively improve the ranking weight.  In addition, as previously mentioned the TD* datasets are sparse in terms of relevant documents for each query: in general, fewer than five percent of documents were found to be relevant.  These two factors may have made the updating process ineffective for a majority of queries; indeed, in our baseline experiment the exploratory list scored more highly than the exploitative list three times in a thousand iterations. \\ \\
In addition, the moving average approach carries several operational risks.  Chief among them, it may overemphasize improvements to the ranking weight vector by both updating it according to a successful exploratory weight and by factoring previously successful exploratory weights into the generation of new exploratory weights.  This effectively counts such weights “twice” when generating a new exploratory weight, and may lead the weight vector to overcompensate after successful explorations.  \\ \\
Although the experiment did not improve on the current listwise learning to rank algorithm, due to its structure and the difficulty of updating the ranking weight in an intelligent and automated manner, there are very few known approaches to the task.  As such, the fact that our approach was unsuccessful may be helpful to scholars seeking to avoid “dead ends” in future research endeavors in this area. 

\section{Analysis}
In general, our findings confirm that both learning methods are able to perform higher than the baseline, and support the emphasis of exploration over exploitation.  This second finding is in contrast to the findings of Hofmann et al [cite], whose results were higher for reinforcement learning models that favored exploitation over exploration, or a near-even balance of the two, for all click models, and especially for the perfect click model. \\ \\
The perfect click model is the least noisy of the click models, yet exploration is most reliably high here, which seems contradictory.  However, it bears noting that both of the TD* datasets are low-relevance datasets – they have very few relevant documents for each query.  This may overemphasize the benefits of exploration as well as the superiority of both models to the baseline, as the benefits of exploration are more apparent when the target relevant documents are sparsely distributed (an exploitative algorithm is likely to have more trouble finding them).  Results may have differed for datasets with a greater proportion of relevant documents to irrelevant documents for each query.  \\ \\
In addition, the reinforcement learning model appears to dominate the active learning model when using a perfect click model, but the reverse is true of the navigational click model, and the models are competitive when using the informational click model.  This may be due to the increased noise of the informational and navigational models over the perfect click model, which, coupled with the sparseness of relevant documents for each query, allows active learning to gain information more effectively by targeting noisy documents to a greater extent than in the reinforcement learning approach, which randomly targets exploratory documents.   \\ \\


\bibliographystyle{abbrv}
\bibliography{sigproc-sp}  % sigproc.bib is the name of the Bibliography in this case

\balancecolumns
% That's all folks!


\end{document}
